# Written 12/20/2018 by Alex Harris
#########################################################
#							#
#			INTRODUCTION			#
#							#
#########################################################
This document serves as a overview for the scripts contained in this folder and will attempt to give an overview of our current NGS pipeline which is based mainly off of the GATK Best Practices for processing raw NGS to versions suitable for downstream analyses. This pipeline is designed for Germline SNPs + Indels and will use the programs BWA-MEM, Samtools, Picard, and the GATK suite. The following will describe each tool and the overview of each script and the version of tool (if applicable) this pipeline currently uses for each step and how to utilize the outputs. This readme will also highlight which outputs will be needed to transfer over to storage to Ketu for analysis.

This Readme is structured in the way the pipeline moves, so there will be repeat sections, such as Picard tools making multiple appearances in this Readme. Note this is not a duplication, but a different tool used within the suites.

All of the scripts being used have an interactive portion in them, meaning that it requires input from the user to initially locate the files to be used. The reason for this is to allow different users to use the same script but are not required to place their files within the Ostrander shared drive. Users can use their personal data drives to store and/or work on files.

Please note, you should copy a version of this folder to your home directory to modify and/or execute scripts in your home directory. This way the base scripts will remain unmodified for users to be able to reference and then modify to suit their needs. This will also eliminate the possibility of multiple people using the same script at the same time and then having multiple temporary versions of the same script and causing a file corruption.

#########################################################
#							#
#			FASTQ				#
#							#
#########################################################
When receiving FASTQs to analysis, the first step is to concatenate the read pairs together which comprise the full sequence results for the sample. There is no script to perform this step, simply the user needs to concatenate like fastq's together using the unix command: cat file1 file2 > combined_file . For really deep sequencing runs, typically you will see 2 fastq files for each individual pair of the paired in run.

For an example of how to perform this step, see below.

There are 4 files:
BT1032_DDPL01639-W_HL7JGCCXY_L7_1.fq.gz
BT1032_DDPL01639-W_HL7JGCCXY_L7_2.fq.gz
BT1032_DDPL01639-W_HL7JGCCXY_L8_1.fq.gz
BT1032_DDPL01639-W_HL7JGCCXY_L8_2.fq.gz

To pair correctly, you would pair files ending in _1 together and ending in _2 together. In more human readable terms, files ending in _1 are "Forward Pairs" and files ending in _2 are "Reverse Pairs".

The command to concatenate is:
cat BT1032_DDPL01639-W_HL7JGCCXY_L7_1.fq.gz BT1032_DDPL01639-W_HL7JGCCXY_L8_1.fq.gz > BT1032_1.fastq.gz

Once you've concatenate all of the fastq files together, those files are then ready to be processed through BWA-MEM.

#########################################################
#							#
#			BWA-MEM				#
#							#
#########################################################
This script will ask the user for the directory containing the files you want to align and then will search the directory for files named "_1.fastq.gz" and then place the basename into an array which will then used in the creation of the swarmfile by iterating across the array using a for loop.

The script makes use of the biowulf scratch partition, which each user has but files located on a users scratch partition will be deleted after about one week after last access. So once the script is run and the swarm is completed then the user has about one week to act on those files and proceed to the next step.

This section also makes use of the CanFam 3.1 FASTA file as a means to align those raw reads to the reference genome. This script will typically last about 4 days.

The output of this script will be a .sam file to be used for Samtools to convert to a .bam file.

#########################################################
#							#
#			Samtools			#
#							#
#########################################################
Script will convert the temporary .sam file to a sorted .bam file and will ask the user for the directory containing the files you want to convert and also where the user wants to output the sorted .bam files to.

This step will generate a hefty amount of temporary files during the sort phase and requires the usage of Biowulf's local scratch partition. This script will also index generated .bam files and is expected to not take longer than 4 days. 

After the generation of the .bam files the user should move to samtools flagstat tool (and assosicated script) to verify that the created .bam files are not truncated.The flagstat script will ask the user where the new sorted .bam's are located at and then run the program on said .bam's and then it will output a text file in the same directory with statistics.

An example of the sorted bam would be: sort_foo.bam & sort_foo.bai
An example of the flagstat file of the sorted bam would be: foo_flagstat.o
You can open the flagstat file in vim or via command line by typing more "filename" on the command prompt

#########################################################
#							#
#			Picard				#
#							#
#########################################################
Picard tools at this step in the pipeline will be to take the newly minted sorted .bam files and then Add/Replace read groups located in the headers of each .bam. This is important downstream when creating the gVCFs. Unless you have a specific reason to change the current read groups in the script, do not change them. The output of the first half of the script will be a bam file that looks like the one below:
	"RG_foo.bam" where RG stands for ReadGroup. This RG bam file is a temporary file and can be deleted once the pipeline is completed.

The second half of the script will look through the RG bam file and then mark duplicates within the bam. The MarkDuplicates tool works by comparing sequences in the 5 prime positions of both reads and read-pairs in a SAM/BAM file. An BARCODE_TAG option is available to facilitate duplicate marking using molecular barcodes. After duplicate reads are collected, the tool differentiates the primary and duplicate reads using an algorithm that ranks reads by the sums of their base-quality scores (default method). When the input is coordinate-sorted, unmapped mates of mapped records and supplementary/secondary alignments are not marked as duplicates. However we have set the tool to not remove duplicates in favor of simply having them marked as duplicates

The tool's main output is a new SAM or BAM file, in which duplicates have been identified in the SAM flags field for each read. Duplicates are marked with the hexadecimal value of 0x0400, which corresponds to a decimal value of 1024.

This part of the script will output a marked duplicate .bam file with the naming scheme of:
	"dedup_foo.bam" where dedup is our modifier stating that this bam has had duplicates marked. This .bam file is to be kept and indexed. DO NOT DELETE THIS BAM.

It will also output a metrics text file	indicating the numbers of duplicates for both single- and paried-end reads. This file will have the scheme of:
	"foo_metrics.txt"
This process has a current wall time set at 4 days.

#########################################################
#							#
#			GATK				#
#							#
#########################################################
"BaseRecalibrator"
First pass of the base quality score recalibration. Generates a recalibration table based on various covariates. The default covariates are read group, reported quality score, machine cycle, and nucleotide context.

This walker generates tables based on specified covariates. It does a by-locus traversal operating only at sites that are in the known sites VCF. We use the CanFam3.1 version 151 dbSNP VCF to use as our known sites. GATK assumes that all reference mismatches we see are therefore errors and indicative of poor base quality. Since there is a large amount of data one can then calculate an empirical probability of error given the particular covariates seen at this site, where p(error) = num mismatches / num observations. The output file is a table (of the several covariate values, num observations, num mismatches, empirical quality score).

#Output
The output of this tool are a recalibration table that is able to be read by any text editor and is used in the following step in applying the Recalibration to the bam files in the pipelne. This table will have the naming schema of:
	"foo_recal4.table"

"ApplyBQSR"
This tool performs the second pass in a two-stage process called Base Quality Score Recalibration (BQSR). Specifically, it recalibrates the base qualities of the input reads based on the recalibration table produced by the BaseRecalibrator tool, and outputs a recalibrated BAM or CRAM file.

The goal of this procedure is to correct for systematic bias that affect the assignment of base quality scores by the sequencer. The first pass consists of calculating error empirically and finding patterns in how error varies with basecall features over all bases. The relevant observations are written to a recalibration table. The second pass consists of applying numerical corrections to each individual basecall based on the patterns identified in the first step (recorded in the recalibration table) and write out the recalibrated data to a new BAM.

#Output
The output of this tool is a calibrated .bam file that is to be archived as it can be used for analysis. It will have the naming schema of:
	"foo_BQSR.bam" - Note that there may be BQSR bam's that appear like: "foo_BQSR4.bam", this is to delineate between BQSR bams that have been created using GATK 3.8.0 vs GATK 4.0.8.1

####
"HaplotypeCaller"
Call germline SNPs and indels via local re-assembly of haplotypes

The HaplotypeCaller is capable of calling SNPs and indels simultaneously via local de-novo assembly of haplotypes in an active region. In other words, whenever the program encounters a region showing signs of variation, it discards the existing mapping information and completely reassembles the reads in that region. This allows the HaplotypeCaller to be more accurate when calling regions that are traditionally difficult to call, for example when they contain different types of variants close to each other. 
In the GVCF workflow used for scalable variant calling in DNA sequence data, HaplotypeCaller runs per-sample to generate an intermediate GVCF (not to be used in final analysis), which can then be used in GenotypeGVCFs for joint genotyping of multiple samples in a very efficient way.
#How HaplotypeCaller works
	#1 Define Active Regions
	The program determines which regions of the genome it needs to operate on (active regions), based on the presence of evidence for variation. 
	#2 Determine haplotypes by assembly of the active region
	For each active region, the program builds a De Bruijn-like graph to reassemble the active region and identifies what are the possible haplotypes present in the data. The program then realigns each haplotype against the reference haplotype using the Smith-Waterman algorithm in order to identify potentially variant sites. 
	#3 Determine likelihoods of the haplotypes given the read data
	For each active region, the program performs a pairwise alignment of each read against each haplotype using the PairHMM algorithm. This produces a matrix of likelihoods of haplotypes given the read data. These likelihoods are then marginalized to obtain the likelihoods of alleles for each potentially variant site given the read data. 
	#4 Assign sample genotypes
	For each potentially variant site, the program applies Bayes' rule, using the likelihoods of alleles given the read data to calculate the likelihoods of each genotype per sample given the read data observed for that sample. The most likely genotype is then assigned to the sample. 

To execute the script, the user will input the desired .bams that need to have variants called on them and the output will be a GVCF with raw unfiltered SNP and indel calls. We are using the GVCF workflow in this pipeline so this intermediate GVCF must have have the tool GenotypeGVCFs run on it and then filtering before further analysis.

#Output
Output of this script are multiple files; the most pertinent file is the GVCF file which will have the naming schema of:
	"foo_g.vcf.gz" - This is the GVCF to be used in processing downstream
The other file is an bam in which assembled haplotypes are written to. Note not every read will be emitted to the bam. This option comes at a hard performance cost of the caller, but it is appropriate if you wish to know why the caller is making specific calls and reads are written out containing an HC tag. See GATK tools for more details. The naming schema for this file is:
	"foo_realigned.bam"

TIP - If you are having issues in generating a GVCF and the error you are coming across is at the Smith-Waterman adjustment step, in the script change the flag --smith-waterman from FASTEST_AVAILABLE to JAVA and then re-run. It may take a bit longer however it is the most compatible flag to run.

####
"GenomicsDBImport"
Import single-sample GVCFs into GenomicsDB before joint genotyping.

The GATK4 Best Practice Workflow for SNP and Indel calling uses GenomicsDBImport to merge GVCFs from multiple samples. GenomicsDBImport offers the same functionality as CombineGVCFs and comes from the Intel-Broad Center for Genomics. The datastore transposes sample-centric variant information across genomic loci to make data more accessible to tools.

Details on GenomicsDB are at https://github.com/Intel-HLS/GenomicsDB/wiki. In brief, GenomicsDB is a utility built on top of TileDB. TileDB is a format for efficiently representing sparse data. Genomics data is typically sparse in that each sample has few variants with respect to the entire reference genome. GenomicsDB contains code to specialize TileDB for genomics applications, such as VCF parsing and INFO field annotation calculation.

This tool is hard to understand as it's unintuitive in its application. But it makes sense after a few runs with the tool and it eventually clicks.
We use this tool to gather all of the experimental GVCFs and then we gather our 155 representative VCF files of dogs from the original 722 Dog Genome Dataset and flatten them into the GenomicsDB workspace. If you are curious as to what the 155 dog list is comprised of, that list resides in the temp folder of the GenomicsDBImport tool script.

When importing, the user is "REQUIRED" to place the full database in an empty directory. I forsee this being the most common error when running this script. You can not add samples to a current database, so each experiment needs to have its separate database. It is NOT RECOMMENDED to maintain these databases as they are very large in size, for example a database containing 243 total samples will cost about 9 TB of disk space.

This process will take upwards of 5+ days to complete. This tool is designed to scale up to 10s of thousands of samples, at the cost of database build speed and disk space. I would envision building a database comprising of the full 722 dataset and then the experimental samples would take longer than 10 days, so it would have to be run on the unlimited node rather than norm. See the Biowulf User Guide at https://hpc.nih.gov/docs/userguide.html for details.

Once completed, you will have 39 folders in total, one for each chromosome (38) and the X chromosome. The database is now ready to be queried or ready for downstream processes.

TIP - If you are only interested in certain chromosomes, for example chr 6 or chr 11 because you are performing a cancer study where the only genes of interest are located on those chromosomes, you can indeed only create a database comprising of just those chromosomes. Simply run the script and do not submit the swarm to the cluster and abort the script when prompted. Then edit the swarmfile by deleting the chromosomes that are not desired. Protip: If you are using VIM to edit the swarmfile type dd to delete the entire line containing undesired chromosomes.

####
"GenotypeVCFs"
Perform joint genotyping on one or more samples pre-called with HaplotypeCaller

This tool is designed to perform joint genotyping on a single input, which may contain one or many samples. In any case, the input samples must possess genotype likelihoods produced by HaplotypeCaller with `-ERC GVCF` or `-ERC BP_RESOLUTION`.

#Input
The GATK4 GenotypeGVCFs tool can take only one input track. Options are 1) a single single-sample GVCF 2) a single multi-sample GVCF created by CombineGVCFs or 3) a GenomicsDB workspace created by GenomicsDBImport. A sample-level GVCF is produced by HaplotypeCaller with the `-ERC GVCF` setting. In the context of the pipeline, we use option 3, using the GenomicsDB workspace to use as the input.

#Caveats
- Only GVCF files produced by HaplotypeCaller (or CombineGVCFs) can be used as input for this tool. Some other programs produce files that they call GVCFs but those lack some important information (accurate genotype likelihoods for every position) that GenotypeGVCFs requires for its operation.
- Cannot take multiple GVCF files in one command.

#Output
The output of this script will be combined VCFs separated out by chromosomes, so if the database contains all 38 chromosomes plus the X chromosomes there should be 39 VCFs ready for the next step which will be to merge the VCFs into one

####
"GatherVCFs"
Gathers multiple VCF files from a scatter operation into a single VCF file. Input files must be supplied in genomic order and must not have events at overlapping positions.

This script will organize all of the scattered VCFs into an order suitable for the tool and then out put a single VCF with a naming schema of:
	"foo.chrAll.vcf.gz" - Where foo will be the name the user gives the combined vcf file

#Input
VCF files that are in genomic order and do not have any overlapping positions.

#Output
A single VCF file. This file is not analysis-ready as it still needs to be filtered.

####
"VariantRecalibrator" and "ApplyRecalibration"
The purpose of variant recalibration is to assign a well-calibrated probability to each variant call in a call set. You can then create highly accurate call sets by filtering based on this single estimate for the accuracy of each call. The approach taken by variant quality score recalibration is to develop a continuous, covarying estimate of the relationship between SNP call annotations (such as QD, MQ, and ReadPosRankSum, for example) and the probability that a SNP is a true genetic variant versus a sequencing or data processing artifact.

This adaptive error model can then be applied to both known and novel variation discovered in the call set of interest to evaluate the probability that each call is real. The score that gets added to the INFO field of each variant is called the VQSLOD. It is the log odds of being a true variant versus being false under the trained Gaussian mixture model.

This tool performs the first pass in a two-stage process called VQSR; the second pass is performed by the ApplyRecalibration tool. In brief, the first pass consists of creating a Gaussian mixture model by looking at the distribution of annotation values over a high quality subset of the input call set, and then scoring all input variants according to the model. The second pass consists of filtering variants based on score cutoffs identified in the first pass. 
